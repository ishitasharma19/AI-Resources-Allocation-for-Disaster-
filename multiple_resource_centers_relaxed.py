# -*- coding: utf-8 -*-
"""Multiple_resource_centers_relaxed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PNh97scHmyQTEWWlMsSG36tDyeLJIM_Q
"""

import numpy as np
import math
import random
import time


class QLearning:
  def __init__( self ):
    self.Q = {}
    self.states = []
    self.actions_per_center = []
    self.actions = []
    self.delivery_costs = []
    self.number_of_affected_areas = int(input("Enter the number of affected area: "))
    self.number_of_resource_centers = int(input("Enter the number of resource centers: "))
    self.demand_per_area = int(input("Enter the demand per time period from each area: "))
    self.capacity_per_center = int(input("Enter the supply per time period: "))
    self.time_periods = int(input("Enter the number of time periods: "))
    self.delivery_cost_per_action = {}
    self.terminal_penalty_per_state = {}
    self.deprivation_cost_per_state = {}
    self.a = 1.5031
    self.b = 0.1172
    self.gamma = 0.6 #discount factor
    self.alpha = 0.8 #learning factor

    self.start_time = time.time()
    self.training_start_time = 0
    self.training_end_time = 0
    self.end_time = 0

    self.convertedActions = []

    self.populateActions()
    self.populateStates()
    self.populateDeliveryCosts()
    self.populateQ()
    self.convertActions()

    print(self.actions)
    print(self.states)

  def populateActions(self):
    initial_action = []
    for i in range (self.number_of_affected_areas):
      initial_action.append(0)
    self.actions_per_center.append(tuple(initial_action))
    self.generateActionsPerCenter(initial_action,0)
    self.generateActions()

  def generateActionsPerCenter(self,action,total_supply):
    if total_supply < self.capacity_per_center:
      for idx,supply in enumerate(action):
        new_action = action.copy()
        new_action[idx] += 1
        if new_action not in self.actions:
          self.actions_per_center.append(tuple(new_action))
          self.generateActionsPerCenter(new_action,total_supply+1)
    else:
      return

  def generateActions(self):
    total_actions = pow(len(self.actions_per_center),self.number_of_resource_centers)
    while len(self.actions)!=total_actions:
        action = random.choices(self.actions_per_center,weights = None, k=self.number_of_resource_centers)
        if action not in self.actions:
          self.actions.append(action)

  def generateNextState(self,state,action):
    next_state = state.copy()
    for area in range (self.number_of_affected_areas):
      relief_provided = 0
      for center in range (self.number_of_resource_centers):
        relief_provided += action[center][area]
      next_state[area] = next_state[area] + self.demand_per_area - relief_provided
    return next_state

  def isValid(self,state):
    #print("checking validity")
    #print(state)
    initial_state = []
    for i in range (self.number_of_affected_areas):
      initial_state.append(0)
    if state == initial_state:
      return True
    for area in state:
      if area>self.time_periods*(self.demand_per_area) or area < -abs(self.time_periods*(self.demand_per_area-(self.capacity_per_center*self.number_of_resource_centers))):
        #print("invalid")
        return False
    return True

  def populateStates(self):
    initial_state = []
    for i in range (self.number_of_affected_areas):
      initial_state.append(0)
    unchecked_states = []
    unchecked_states.append(initial_state)
    while len(unchecked_states)!=0:
      current_state = unchecked_states[0]
      valid = self.isValid(current_state)
      #print("here at checking validity")
      #print(current_state)
      if valid:
        for action in self.actions:
          next_state = self.generateNextState(current_state,action)
          if next_state not in unchecked_states and next_state not in self.states:
            unchecked_states.append(next_state)
        self.states.append(current_state)
      unchecked_states.pop(0)

  def populateDeliveryCosts(self):
    even_center = 1
    odd_center = 1
    for center in range (self.number_of_resource_centers):
      costs = []
      if center%2==0:
        for area in range (self.number_of_affected_areas):
          costs.append((area+1+even_center)*5)
        even_center += 1
      else:
        for area in range (self.number_of_affected_areas):
          costs.append((area+1-odd_center)*5)
        odd_center += 1
      self.delivery_costs.append(costs)

  def computeDeprivationCost(self,state):
    if tuple(state) in self.deprivation_cost_per_state.keys():
      return self.deprivation_cost_per_state[tuple(state)]
    deprivation = 0
    for area in state:
      deprivation += (math.exp(self.b*10)-1)*(math.exp(self.a+(self.b*area*10)))
    self.deprivation_cost_per_state[tuple(state)] = deprivation
    return deprivation

  def computeTerminalPenaltyCost(self,state):
    return self.computeDeprivationCost(state)

  def computeDeliveryCostPerAction(self,action):
    # print(action)
    # print(tuple(action))
    if tuple(action) in self.delivery_cost_per_action.keys():
      return self.delivery_cost_per_action[tuple(action)]
    cost = 0
    for center,action_per_center in enumerate(action):
      for area,supply in enumerate(action_per_center):
        cost += self.delivery_costs[center][area]*supply
    self.delivery_cost_per_action[tuple(action)] = cost
    return cost


  def populateQ(self):
    for state in self.states:
      self.Q[tuple(state)] = np.zeros(len(self.actions))

  def training(self):
    K = max(2000,3*len(self.states))
    for k in range (K):
      epsilon = 0.5/(1+math.exp((10*(k-(0.4*K)))/K))
      current_state = random.choice(self.states)
      for t in range (self.time_periods):
        selection = random.choices([0,1],weights = (1-epsilon,epsilon))
        #print(selection)
        if selection[0]==0:
          current_action = self.actions[np.argmax(self.Q[tuple(current_state)])]
        else:
          current_action = random.choice(self.actions)
        next_state = self.generateNextState(current_state,current_action)
        if t==0:
          reward = -1*self.computeDeliveryCostPerAction(current_action) - self.computeDeprivationCost(current_state) - self.computeDeprivationCost(next_state)
        elif t==self.time_periods-1:
          reward = -1*self.computeDeliveryCostPerAction(current_action) - self.computeTerminalPenaltyCost(next_state)
        else:
          reward = -1*self.computeDeliveryCostPerAction(current_action) - self.computeDeprivationCost(next_state)

        if next_state not in self.states:
          TD = reward - self.Q[tuple(current_state)][self.actions.index(current_action)]
        else:
          TD = reward + self.gamma*self.Q[tuple(next_state)][np.argmax(self.Q[tuple(next_state)])] - self.Q[tuple(current_state)][self.actions.index(current_action)]
        self.Q[tuple(current_state)][self.actions.index(current_action)] += self.alpha*TD
        current_state = next_state
        if current_state not in self.states:
          current_state = random.choice(self.states)

  def getOptimalPath(self,current_state,route_state,route_action,total_reward):
    #print('here at getting path')
    while(len(route_state)!=self.time_periods+1):
      #print("less go")
      #print(current_state)
      #print(np.argmax(self.Q[tuple(start_state)]))
      current_action = self.actions[np.argmax(self.Q[tuple(current_state)])]
      #print(current_action)
      #print(current_action)
      next_state = self.generateNextState(current_state,current_action)
      #print(next_state)
      if self.isValid(next_state) != -1:
        route_state.append(next_state)
        route_action.append(current_action)
        total_reward += self.Q[tuple(current_state)][self.actions.index(current_action)]
      else:
        print("some error occurred")
        return (0)
      #print(next_state)

      current_state = next_state
    return (route_state,route_action,total_reward)

  def returnTime(self):
    return self.end_time-self.start_time

  def returnTrainingTime(self):
    return self.training_end_time - self.training_start_time

  def returnResultTime(self):
    return self.end_time - self.training_end_time

  def solve(self):
    start_state = []
    for i in range (self.number_of_affected_areas):
      start_state.append(0)
    self.training_start_time = time.time()
    self.training()
    self.training_end_time = time.time()
    #print(self.Q)
    route_state = []
    route_state.append(start_state)
    route_action = []
    result = self.getOptimalPath(start_state,route_state,route_action,0)
    self.end_time = time.time()
    return result

  def initialState(self):
    state = []
    for i in range (self.number_of_affected_areas):
      state.append(0)
    return state

  def convertToOriginal(self,action):
    new_action = []
    i = 0
    while i!=len(action):
      areas = []
      for j in range (self.number_of_affected_areas):
        areas.append(action[i])
        i += 1
      new_action.append(tuple(areas))
    return new_action

  def step(self,state,action):
    action = self.convertToOriginal(action)
    next_state = self.generateNextState(state,action)
    reward = -1*self.computeDeliveryCostPerAction(action)- self.computeDeprivationCost(next_state) - self.computeDeprivationCost(state)
    terminal_state = not self.isValid(next_state)
    return (next_state,reward,terminal_state)

  def convertActions(self):
    for action in self.actions:
      converted_action = []
      for i in range (len(action)):
        for j in range (len(action[i])):
          converted_action.append(action[i][j])
      self.convertedActions.append(converted_action)

qLearning = QLearning()
result = qLearning.solve()
total_time = qLearning.returnTime()
training_time = qLearning.returnTrainingTime()
result_time = qLearning.returnResultTime()
print(result)
print(total_time)
print(training_time)
print(result_time)

"""
A Minimal Deep Q-Learning Implementation (minDQN)

Running this code will render the agent solving the CartPole environment using OpenAI gym. Our Minimal Deep Q-Network is approximately 150 lines of code. In addition, this implementation uses Tensorflow and Keras and should generally run in less than 15 minutes.

Usage: python3 minDQN.py
"""

import gym
import tensorflow as tf
import numpy as np
from tensorflow import keras

from collections import deque
import time
import random

RANDOM_SEED = 5
tf.random.set_seed(RANDOM_SEED)

#env = gym.make('CartPole-v1')
env= QLearning()
#env.seed(RANDOM_SEED)
#np.random.seed(RANDOM_SEED)

#print("Action Space: {}".format(env.action_space))
#print("State space: {}".format(env.observation_space))
#print("Action Space: {}".format(len(env.actions)))
#print("State space: {}".format(len(env.states)))

#print(type(env.action_space))
#print(type(env.observation_space))
# print(np.shape(env.states))
# print(np.shape(env.convertedActions))
#print(env.convertedActions[0])
#print(env.observation_space.shape)
#print(env.action_space.n)

# An episode a full game
#train_episodes = 300
train_episodes = max(100, int(len(env.states)/64))
#print(train_episodes)
test_episodes = 100
#print(test_episodes)

def agent(state_shape, action_shape):
    """ The agent maps X-states to Y-actions
    e.g. The neural network output is [.1, .7, .1, .3]
    The highest value 0.7 is the Q-Value.
    The index of the highest action (0.7) is action #1.
    """
    #print("here ate agent")
    learning_rate = 0.001
    init = tf.keras.initializers.HeUniform()
    model = keras.Sequential()
    #print("1")
    model.add(keras.layers.Dense(24, input_shape=(state_shape,), activation='relu', kernel_initializer=init))
    #print("2")
    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))
    #print("3")
    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))
    #print("4")
    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])
    #print("5")
    return model

def get_qs(model, state, step):
    return model.predict(state.reshape([1, state.shape[0]]))[0]

def train(env, replay_memory, model, target_model, done):
    learning_rate = 0.7 # Learning rate
    discount_factor = 0.618

    MIN_REPLAY_SIZE = 1000
    if len(replay_memory) < MIN_REPLAY_SIZE:
        return

    batch_size = 64 * 2
    mini_batch = random.sample(replay_memory, batch_size)
    current_states = np.array([transition[0] for transition in mini_batch])
    current_qs_list = model.predict(current_states)
    new_current_states = np.array([transition[3] for transition in mini_batch])
    future_qs_list = target_model.predict(new_current_states)

    X = []
    Y = []
    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):
        if not done:
            max_future_q = reward + discount_factor * np.max(future_qs_list[index])
        else:
            max_future_q = reward

        current_qs = current_qs_list[index]
        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q

        X.append(observation)
        Y.append(current_qs)
    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)

import time
def main():
    start_time = time.time()
    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start
    max_epsilon = 1 # You can't explore more than 100% of the time
    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time
    decay = 0.01

    # 1. Initialize the Target and Main models
    # Main Model (updated every 4 steps)
    #print("i came till here")
    #model = agent(env.observation_space.shape, env.action_space.n)
    model = agent(env.number_of_affected_areas, len(env.actions))
    # Target Model (updated every 100 steps)
    #target_model = agent(env.observation_space.shape, env.action_space.n)
    #print("trying for target model")
    target_model = agent(env.number_of_affected_areas, len(env.actions))
    target_model.set_weights(model.get_weights())

    replay_memory = deque(maxlen=50_000)

    target_update_counter = 0

    # X = states, y = actions
    X = []
    y = []

    steps_to_update_target_model = 0

    for episode in range(train_episodes):
        total_training_rewards = 0
        #observation = env.reset()
        observation = env.initialState()
        done = False
        while not done:
            steps_to_update_target_model += 1


            random_number = np.random.rand()
            # 2. Explore using the Epsilon Greedy Exploration Strategy
            if random_number <= epsilon:
                # Explore
                #action = env.action_space.sample()
                action = random.choice(env.convertedActions)
                #print("printing action")
                #print(action)
            else:
                # Exploit best known action
                # model dims are (batch, env.observation_space.n)
                encoded = np.array(observation)
                #print("printing encoded")
                #print(encoded)
                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])
                #print("printing enocded_reshaped")
                #print(encoded_reshaped)
                predicted = model.predict(encoded_reshaped).flatten()
                #print("printing predicted")
                #print(predicted)
                action = np.argmax(predicted)
                action = env.convertedActions[action]
                #print("printing action")
                #print(action)
            #new_observation, reward, done, info = env.step(action)
            new_observation, reward, done = env.step(observation,action)
            #print("printing new_obs, reward, and done or not")
            #print(new_observation)
            #print(reward)
            #print(done)
            replay_memory.append([observation, action, reward, new_observation, done])

            # 3. Update the Main Network using the Bellman Equation
            if steps_to_update_target_model % 4 == 0 or done:
                train(env, replay_memory, model, target_model, done)

            observation = new_observation
            total_training_rewards += reward

            if done:
                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))
                total_training_rewards += 1

                if steps_to_update_target_model >= 100:
                    print('Copying main network weights to the target network weights')
                    target_model.set_weights(model.get_weights())
                    steps_to_update_target_model = 0
                break

        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)

    end_time = time.time()
    print("training time is")
    print(end_time-start_time)
    return (model,target_model)

def find_path(model,env):
  path = []
  actions_to_be_taken = []
  net_reward  = 0
  current_state = env.initialState()
  path.append(current_state)
  while len(path)!=env.time_periods+1:
    encoded = np.array(current_state)
    encoded_reshaped = encoded.reshape([1, encoded.shape[0]])
    predicted = model.predict(encoded_reshaped).flatten()
    action = np.argmax(predicted)
    actions_to_be_taken.append(env.actions[action])
    action = env.convertedActions[action]
    new_observation, reward, done = env.step(current_state,action)
    net_reward += reward

    current_state = new_observation
    path.append(current_state)

  return (path,actions_to_be_taken,net_reward)

model,target_model = main()

result = find_path(model,env)

print(result)

